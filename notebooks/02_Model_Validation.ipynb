{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Risk Model Validation\n",
    "\n",
    "This notebook demonstrates the comprehensive validation process for credit risk models following regulatory guidelines and industry best practices. We'll cover:\n",
    "\n",
    "1. Loading a previously developed model\n",
    "2. Performance testing and benchmarking\n",
    "3. Stability assessment\n",
    "4. Sensitivity analysis\n",
    "5. Out-of-time validation\n",
    "6. Validation report generation\n",
    "\n",
    "This validation framework is designed to align with regulatory requirements for model risk management in banking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "# Add the parent directory to path to import local modules\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Import local modules\n",
    "from src.data_processing.generate_synthetic_data import generate_credit_data\n",
    "from src.data_processing.preprocess import preprocess_data, create_feature_pipeline\n",
    "from src.model_development.models import CreditRiskModel\n",
    "from src.model_validation.validator import ModelValidator, validate_model\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Load configuration\n",
    "with open('../config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Model and Data\n",
    "\n",
    "First, we'll load the model we developed in the previous notebook, along with our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load a previously saved model, or train a new one if not available\n",
    "model_type = 'gradient_boosting'  # Change this to match your best model from notebook 1\n",
    "model_path = f'../models/credit_risk_{model_type}.pkl'\n",
    "\n",
    "try:\n",
    "    # Load the model if it exists\n",
    "    credit_model = CreditRiskModel.load_model(model_path, model_type)\n",
    "    print(f\"Loaded model from {model_path}\")\n",
    "except (FileNotFoundError, pickle.UnpicklingError):\n",
    "    print(f\"Couldn't load model from {model_path}. Training a new model...\")\n",
    "    # Generate data and train a model\n",
    "    data = generate_credit_data(n_samples=10000, random_seed=42)\n",
    "    target_variable = config['data']['target_variable']\n",
    "    \n",
    "    # Split data\n",
    "    train_ratio = 0.7\n",
    "    train_size = int(len(data) * train_ratio)\n",
    "    train_data = data.iloc[:train_size]\n",
    "    test_data = data.iloc[train_size:]\n",
    "    \n",
    "    # Preprocess data\n",
    "    pipeline = create_feature_pipeline(config, target_col=target_variable)\n",
    "    X_train, y_train = preprocess_data(train_data, config, target_col=target_variable, is_training=True)\n",
    "    X_test, y_test = preprocess_data(test_data, config, target_col=target_variable, is_training=False, preprocessing_pipeline=pipeline)\n",
    "    \n",
    "    # Train model\n",
    "    from src.model_development.models import train_model\n",
    "    model = train_model(X_train, y_train, model_type=model_type)\n",
    "    credit_model = CreditRiskModel(model_type, model=model)\n",
    "    \n",
    "    # Save the model\n",
    "    os.makedirs('../models', exist_ok=True)\n",
    "    credit_model.save_model(model_path)\n",
    "    print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "try:\n",
    "    # Try to load saved datasets\n",
    "    train_data = pd.read_csv('../data/credit_data_train.csv')\n",
    "    test_data = pd.read_csv('../data/credit_data_test.csv')\n",
    "    val_data = pd.read_csv('../data/credit_data_validation.csv')\n",
    "    print(\"Loaded existing datasets\")\n",
    "except FileNotFoundError:\n",
    "    # Generate new datasets if not found\n",
    "    print(\"Generating new synthetic datasets\")\n",
    "    from src.data_processing.generate_synthetic_data import split_and_save_data\n",
    "    data = generate_credit_data(n_samples=10000, random_seed=42)\n",
    "    train_data, test_data, val_data = split_and_save_data(data, output_dir='../data')\n",
    "\n",
    "# Display dataset sizes\n",
    "print(f\"Training data: {train_data.shape[0]} samples\")\n",
    "print(f\"Test data: {test_data.shape[0]} samples\")\n",
    "print(f\"Validation data: {val_data.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for validation\n",
    "target_variable = config['data']['target_variable']\n",
    "\n",
    "# Create feature pipeline\n",
    "pipeline = create_feature_pipeline(config, target_col=target_variable)\n",
    "\n",
    "# Preprocess all datasets\n",
    "X_train, y_train = preprocess_data(train_data, config, target_col=target_variable, is_training=True)\n",
    "X_test, y_test = preprocess_data(test_data, config, target_col=target_variable, is_training=False, preprocessing_pipeline=pipeline)\n",
    "X_val, y_val = preprocess_data(val_data, config, target_col=target_variable, is_training=False, preprocessing_pipeline=pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Model Validator\n",
    "\n",
    "Now we'll create a validator object to perform comprehensive validation of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model validator\n",
    "validator = ModelValidator(credit_model.model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Performance Testing\n",
    "\n",
    "First, let's evaluate the model's performance on different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run performance testing\n",
    "performance_results = validator.performance_testing(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Display results\n",
    "print(\"Performance Testing Results:\\n\")\n",
    "for dataset, metrics in performance_results.items():\n",
    "    print(f\"{dataset.upper()} SET METRICS:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess discrimination metrics\n",
    "discrimination_results = validator.assess_discrimination(X_test, y_test)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(discrimination_results['fpr'], discrimination_results['tpr'], \n",
    "         label=f\"ROC Curve (AUC = {discrimination_results['auc']:.4f})\")\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print KS statistic\n",
    "print(f\"KS Statistic: {discrimination_results['ks_statistic']:.4f}\")\n",
    "print(f\"Gini Coefficient: {discrimination_results['gini']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess calibration\n",
    "calibration_results = validator.assess_calibration(X_test, y_test)\n",
    "\n",
    "# Plot calibration curve\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(calibration_results['mean_predicted_probs'], calibration_results['observed_probs'], 'o-', \n",
    "         label=f\"Calibration Curve (Brier Score = {calibration_results['brier_score']:.4f})\")\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
    "plt.xlabel('Mean Predicted Probability')\n",
    "plt.ylabel('Observed Probability')\n",
    "plt.title('Calibration Plot')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Display expected calibration error\n",
    "print(f\"Expected Calibration Error: {calibration_results['calibration_error']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Benchmark Comparison\n",
    "\n",
    "Now let's compare our model against simpler benchmark models to ensure it adds sufficient value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run benchmark comparison\n",
    "benchmark_results = validator.benchmark_comparison(X_test, y_test)\n",
    "\n",
    "# Display benchmark results\n",
    "benchmark_df = pd.DataFrame(benchmark_results['comparison'])\n",
    "benchmark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot benchmark comparison\n",
    "metrics_to_plot = ['auc', 'accuracy', 'f1_score']\n",
    "benchmark_data = benchmark_df[['model'] + metrics_to_plot]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(metrics_to_plot), figsize=(15, 5))\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    ax = axes[i]\n",
    "    sns.barplot(x='model', y=metric, data=benchmark_data, ax=ax)\n",
    "    ax.set_title(f'Comparison of {metric.upper()}')\n",
    "    ax.set_xlabel('')\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stability Testing\n",
    "\n",
    "Next, we'll assess the stability of the model's performance across different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run stability testing\n",
    "stability_results = validator.stability_testing(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Display PSI for each feature\n",
    "print(\"Population Stability Index (PSI) for each feature:\")\n",
    "psi_df = pd.DataFrame(stability_results['feature_psi'].items(), columns=['Feature', 'PSI'])\n",
    "psi_df = psi_df.sort_values('PSI', ascending=False)\n",
    "psi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PSI for top features\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_psi = psi_df.head(15)\n",
    "bars = plt.bar(top_psi['Feature'], top_psi['PSI'])\n",
    "\n",
    "# Color bars based on PSI thresholds\n",
    "for i, bar in enumerate(bars):\n",
    "    psi_value = top_psi.iloc[i]['PSI']\n",
    "    if psi_value < 0.1:\n",
    "        bar.set_color('green')\n",
    "    elif psi_value < 0.2:\n",
    "        bar.set_color('orange')\n",
    "    else:\n",
    "        bar.set_color('red')\n",
    "\n",
    "plt.axhline(y=0.1, color='green', linestyle='--', label='Low Shift (PSI=0.1)')\n",
    "plt.axhline(y=0.2, color='red', linestyle='--', label='High Shift (PSI=0.2)')\n",
    "plt.title('Population Stability Index (PSI) by Feature')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('PSI Value')\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display performance stability\n",
    "print(\"Performance Stability Metrics:\")\n",
    "for metric, value in stability_results['performance_stability'].items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sensitivity Analysis\n",
    "\n",
    "Now we'll analyze how sensitive the model is to changes in key input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run sensitivity analysis\n",
    "sensitivity_results = validator.sensitivity_analysis(X_test, y_test)\n",
    "\n",
    "# Display feature sensitivity\n",
    "sensitivity_df = pd.DataFrame(sensitivity_results['feature_sensitivity'])\n",
    "sensitivity_df.sort_values('sensitivity_score', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sensitivity for top features\n",
    "top_sensitivity = sensitivity_df.sort_values('sensitivity_score', ascending=False).head(10)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "bars = plt.barh(top_sensitivity['feature'], top_sensitivity['sensitivity_score'])\n",
    "\n",
    "# Color bars based on sensitivity thresholds\n",
    "for i, bar in enumerate(bars):\n",
    "    score = top_sensitivity.iloc[i]['sensitivity_score']\n",
    "    if score < 0.1:\n",
    "        bar.set_color('green')\n",
    "    elif score < 0.3:\n",
    "        bar.set_color('orange')\n",
    "    else:\n",
    "        bar.set_color('red')\n",
    "\n",
    "plt.title('Feature Sensitivity Analysis')\n",
    "plt.xlabel('Sensitivity Score')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed plot for most sensitive feature\n",
    "most_sensitive_feature = sensitivity_df.iloc[sensitivity_df['sensitivity_score'].idxmax()]['feature']\n",
    "feature_detail = sensitivity_results['detailed_sensitivity'][most_sensitive_feature]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(feature_detail['perturbation_factors'], feature_detail['predictions'], marker='o')\n",
    "plt.title(f'Sensitivity Detail for {most_sensitive_feature}')\n",
    "plt.xlabel('Perturbation Factor')\n",
    "plt.ylabel('Average Predicted Probability')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Validate Out-of-Time Performance\n",
    "\n",
    "To simulate out-of-time validation, we'll create a dataset with time-based drift and evaluate performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data with drift to simulate future data\n",
    "def generate_future_data(n_samples=2000, base_data=None, drift_factor=1.2):\n",
    "    \"\"\"Generate data with drift to simulate future data.\"\"\"\n",
    "    if base_data is None:\n",
    "        future_data = generate_credit_data(n_samples=n_samples, random_seed=100)  # Different seed\n",
    "    else:\n",
    "        # Create copy of base data\n",
    "        future_data = base_data.sample(n_samples, replace=True).reset_index(drop=True)\n",
    "        \n",
    "        # Apply drift to numeric features\n",
    "        numeric_cols = future_data.select_dtypes(include=['float64', 'int64']).columns\n",
    "        target_col = config['data']['target_variable']\n",
    "        numeric_features = [col for col in numeric_cols if col != target_col]\n",
    "        \n",
    "        # Apply drift to specific features\n",
    "        if 'income' in future_data.columns:\n",
    "            future_data['income'] = future_data['income'] * drift_factor\n",
    "            \n",
    "        if 'debt_to_income' in future_data.columns:\n",
    "            future_data['debt_to_income'] = future_data['debt_to_income'] * drift_factor * 0.9\n",
    "        \n",
    "        # Apply some random noise to all numeric features\n",
    "        for col in numeric_features:\n",
    "            noise = np.random.normal(1, 0.1, size=len(future_data))\n",
    "            future_data[col] = future_data[col] * noise\n",
    "    \n",
    "    return future_data\n",
    "\n",
    "# Generate future data\n",
    "future_data = generate_future_data(n_samples=2000, base_data=data, drift_factor=1.2)\n",
    "\n",
    "# Preprocess future data\n",
    "X_future, y_future = preprocess_data(\n",
    "    future_data, \n",
    "    config, \n",
    "    target_col=target_variable, \n",
    "    is_training=False, \n",
    "    preprocessing_pipeline=pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on future data\n",
    "future_metrics = credit_model.evaluate(X_future, y_future)\n",
    "print(\"Future Data Performance:\")\n",
    "for metric, value in future_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "# Compare with test data performance\n",
    "test_metrics = credit_model.evaluate(X_test, y_test)\n",
    "print(\"\\nPerformance Delta (Future - Test):\")\n",
    "for metric in future_metrics.keys():\n",
    "    delta = future_metrics[metric] - test_metrics[metric]\n",
    "    print(f\"{metric}: {delta:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate PSI between test and future data\n",
    "from src.model_validation.validator import calculate_psi\n",
    "\n",
    "# Get model predictions\n",
    "test_probs = credit_model.predict_proba(X_test)\n",
    "future_probs = credit_model.predict_proba(X_future)\n",
    "\n",
    "# Calculate PSI for score distributions\n",
    "score_psi = calculate_psi(pd.Series(test_probs), pd.Series(future_probs))\n",
    "print(f\"PSI for score distributions: {score_psi:.4f}\")\n",
    "\n",
    "# Interpret PSI\n",
    "if score_psi < 0.1:\n",
    "    interpretation = \"No significant shift in score distribution\"\n",
    "elif score_psi < 0.2:\n",
    "    interpretation = \"Moderate shift in score distribution\"\n",
    "else:\n",
    "    interpretation = \"Significant shift in score distribution - model may need to be retrained\"\n",
    "    \n",
    "print(f\"Interpretation: {interpretation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot score distributions\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(test_probs, bins=20, alpha=0.5, label='Test Data Scores')\n",
    "plt.hist(future_probs, bins=20, alpha=0.5, label='Future Data Scores')\n",
    "plt.title(f'Score Distribution Comparison (PSI = {score_psi:.4f})')\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generate Validation Report\n",
    "\n",
    "Finally, let's generate a comprehensive validation report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run full validation and generate report\n",
    "validation_results = validate_model(\n",
    "    credit_model.model, \n",
    "    X_train, y_train, \n",
    "    X_test, y_test, \n",
    "    X_val=X_val, y_val=y_val, \n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Create validation output directory\n",
    "validation_dir = '../reports/validation'\n",
    "os.makedirs(validation_dir, exist_ok=True)\n",
    "\n",
    "# Generate report\n",
    "report_path = os.path.join(validation_dir, f'validation_report_{datetime.now().strftime(\"%Y%m%d\")}.md')\n",
    "validator.generate_report(report_path)\n",
    "\n",
    "print(f\"Validation report generated at: {report_path}\")\n",
    "\n",
    "# Generate validation visualizations\n",
    "validator.plot_validation_results(validation_dir)\n",
    "print(f\"Validation visualizations saved to: {validation_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "In this notebook, we've conducted a comprehensive validation of our credit risk model following regulatory guidelines:\n",
    "\n",
    "1. We assessed the model's performance using appropriate discrimination and calibration metrics\n",
    "2. We compared the model against simpler benchmarks to ensure it adds value\n",
    "3. We evaluated the model's stability across different datasets\n",
    "4. We performed sensitivity analysis to understand how the model responds to changes in input variables\n",
    "5. We simulated out-of-time validation to assess the model's robustness to data drift\n",
    "6. We generated a comprehensive validation report\n",
    "\n",
    "This validation framework helps ensure that our model is accurate, stable, and robust, meeting the requirements for model risk management in a banking environment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}