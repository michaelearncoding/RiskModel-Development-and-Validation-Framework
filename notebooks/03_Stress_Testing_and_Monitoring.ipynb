{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Risk Model Stress Testing and Monitoring\n",
    "\n",
    "This notebook demonstrates how to conduct stress testing and ongoing monitoring for credit risk models. We'll cover:\n",
    "\n",
    "1. Loading a previously validated model\n",
    "2. Applying stress scenarios to evaluate model resilience\n",
    "3. Identifying vulnerable customer segments\n",
    "4. Setting up ongoing model monitoring\n",
    "5. Detecting performance degradation and data drift\n",
    "6. Generating monitoring reports and alerts\n",
    "\n",
    "These practices are critical for effective model risk management and ensuring models remain reliable under adverse conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "# Add the parent directory to path to import local modules\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Import local modules\n",
    "from src.data_processing.generate_synthetic_data import generate_credit_data\n",
    "from src.data_processing.preprocess import preprocess_data, create_feature_pipeline\n",
    "from src.model_development.models import CreditRiskModel\n",
    "from src.stress_testing.stress_tester import StressTester, run_stress_test\n",
    "from src.monitoring.monitor import ModelMonitor, simulate_monitoring_over_time\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Load configuration\n",
    "with open('../config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Model and Data\n",
    "\n",
    "First, we'll load the model we developed and validated in previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load a previously saved model, or train a new one if not available\n",
    "model_type = 'gradient_boosting'  # Change this to match your best model from notebook 1\n",
    "model_path = f'../models/credit_risk_{model_type}.pkl'\n",
    "\n",
    "try:\n",
    "    # Load the model if it exists\n",
    "    credit_model = CreditRiskModel.load_model(model_path, model_type)\n",
    "    print(f\"Loaded model from {model_path}\")\n",
    "except (FileNotFoundError, pickle.UnpicklingError):\n",
    "    print(f\"Couldn't load model from {model_path}. Training a new model...\")\n",
    "    # Generate data and train a model\n",
    "    data = generate_credit_data(n_samples=10000, random_seed=42)\n",
    "    target_variable = config['data']['target_variable']\n",
    "    \n",
    "    # Split data\n",
    "    train_ratio = 0.7\n",
    "    train_size = int(len(data) * train_ratio)\n",
    "    train_data = data.iloc[:train_size]\n",
    "    test_data = data.iloc[train_size:]\n",
    "    \n",
    "    # Preprocess data\n",
    "    pipeline = create_feature_pipeline(config, target_col=target_variable)\n",
    "    X_train, y_train = preprocess_data(train_data, config, target_col=target_variable, is_training=True)\n",
    "    X_test, y_test = preprocess_data(test_data, config, target_col=target_variable, is_training=False, preprocessing_pipeline=pipeline)\n",
    "    \n",
    "    # Train model\n",
    "    from src.model_development.models import train_model\n",
    "    model = train_model(X_train, y_train, model_type=model_type)\n",
    "    credit_model = CreditRiskModel(model_type, model=model)\n",
    "    \n",
    "    # Save the model\n",
    "    os.makedirs('../models', exist_ok=True)\n",
    "    credit_model.save_model(model_path)\n",
    "    print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "try:\n",
    "    # Try to load saved datasets\n",
    "    train_data = pd.read_csv('../data/credit_data_train.csv')\n",
    "    test_data = pd.read_csv('../data/credit_data_test.csv')\n",
    "    print(\"Loaded existing datasets\")\n",
    "except FileNotFoundError:\n",
    "    # Generate new datasets if not found\n",
    "    print(\"Generating new synthetic datasets\")\n",
    "    from src.data_processing.generate_synthetic_data import split_and_save_data\n",
    "    data = generate_credit_data(n_samples=10000, random_seed=42)\n",
    "    train_data, test_data, _ = split_and_save_data(data, output_dir='../data')\n",
    "\n",
    "# Display dataset sizes\n",
    "print(f\"Training data: {train_data.shape[0]} samples\")\n",
    "print(f\"Test data: {test_data.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for stress testing and monitoring\n",
    "target_variable = config['data']['target_variable']\n",
    "\n",
    "# Create feature pipeline\n",
    "pipeline = create_feature_pipeline(config, target_col=target_variable)\n",
    "\n",
    "# Preprocess datasets\n",
    "X_train, y_train = preprocess_data(train_data, config, target_col=target_variable, is_training=True)\n",
    "X_test, y_test = preprocess_data(test_data, config, target_col=target_variable, is_training=False, preprocessing_pipeline=pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stress Testing\n",
    "\n",
    "Now we'll perform stress testing to evaluate how the model performs under adverse economic conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize stress tester\n",
    "stress_tester = StressTester(credit_model.model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run stress test across all defined scenarios\n",
    "stress_results = stress_tester.run_stress_test(X_test, y_test)\n",
    "\n",
    "# Display results summary\n",
    "print(\"Stress Testing Results Summary:\\n\")\n",
    "for scenario, results in stress_results['scenario_results'].items():\n",
    "    print(f\"Scenario: {scenario}\")\n",
    "    print(f\"  Average Default Probability: {results['avg_default_prob']:.4f}\")\n",
    "    print(f\"  Default Rate Increase: {results['default_rate_increase']:.2f}x\")\n",
    "    print(f\"  Scenario Severity Rating: {results['severity_rating']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot default probability distribution across scenarios\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for scenario, results in stress_results['scenario_results'].items():\n",
    "    bins = results['probability_distribution']['bins']\n",
    "    freqs = results['probability_distribution']['frequencies']\n",
    "    plt.plot(bins, freqs, label=scenario, alpha=0.7)\n",
    "    \n",
    "plt.title('Default Probability Distribution by Scenario')\n",
    "plt.xlabel('Default Probability')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate capital requirements under different scenarios\n",
    "capital_results = stress_tester.calculate_capital_requirements(X_test, portfolio_size=10000000, lgd=0.6)\n",
    "\n",
    "# Convert to DataFrame for display\n",
    "capital_df = pd.DataFrame([\n",
    "    {\n",
    "        'scenario': scenario,\n",
    "        'expected_loss': results['expected_loss'],\n",
    "        'unexpected_loss': results['unexpected_loss'],\n",
    "        'total_capital': results['total_capital_required'],\n",
    "        'capital_increase': results['capital_increase']\n",
    "    }\n",
    "    for scenario, results in capital_results.items()\n",
    "])\n",
    "\n",
    "# Format as currency and percentage\n",
    "for col in ['expected_loss', 'unexpected_loss', 'total_capital']:\n",
    "    capital_df[col] = capital_df[col].apply(lambda x: f\"${x:,.2f}\")\n",
    "    \n",
    "capital_df['capital_increase'] = capital_df['capital_increase'].apply(lambda x: f\"{x:.2f}x\")\n",
    "\n",
    "capital_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify vulnerable segments\n",
    "vulnerable_segments = stress_tester.identify_vulnerable_segments(X_test, y_test)\n",
    "\n",
    "# Display most vulnerable segments\n",
    "vulnerability_df = pd.DataFrame([\n",
    "    {\n",
    "        'segment': segment,\n",
    "        'baseline_default_rate': results['baseline_default_rate'],\n",
    "        'stressed_default_rate': results['severe_recession_default_rate'],\n",
    "        'default_increase': results['default_increase'],\n",
    "        'vulnerability_rating': results['vulnerability_rating'],\n",
    "        'segment_size': results['segment_size']\n",
    "    }\n",
    "    for segment, results in vulnerable_segments['segments'].items()\n",
    "])\n",
    "\n",
    "# Sort by vulnerability\n",
    "vulnerability_df = vulnerability_df.sort_values('default_increase', ascending=False)\n",
    "\n",
    "# Format percentages\n",
    "vulnerability_df['baseline_default_rate'] = vulnerability_df['baseline_default_rate'].apply(lambda x: f\"{x:.2%}\")\n",
    "vulnerability_df['stressed_default_rate'] = vulnerability_df['stressed_default_rate'].apply(lambda x: f\"{x:.2%}\")\n",
    "vulnerability_df['default_increase'] = vulnerability_df['default_increase'].apply(lambda x: f\"{x:.2f}x\")\n",
    "\n",
    "vulnerability_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top vulnerable segments\n",
    "top_segments = vulnerability_df.head(5)\n",
    "segment_names = top_segments['segment'].tolist()\n",
    "baseline_rates = [float(rate.strip('%'))/100 for rate in top_segments['baseline_default_rate']]\n",
    "stressed_rates = [float(rate.strip('%'))/100 for rate in top_segments['stressed_default_rate']]\n",
    "\n",
    "# Create plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "width = 0.35\n",
    "x = np.arange(len(segment_names))\n",
    "\n",
    "baseline_bars = ax.bar(x - width/2, baseline_rates, width, label='Baseline', color='skyblue')\n",
    "stressed_bars = ax.bar(x + width/2, stressed_rates, width, label='Severe Recession', color='salmon')\n",
    "\n",
    "ax.set_ylabel('Default Rate')\n",
    "ax.set_title('Most Vulnerable Segments: Baseline vs Stressed Default Rates')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(segment_names, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "\n",
    "# Add data labels\n",
    "def add_labels(bars):\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.1%}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "add_labels(baseline_bars)\n",
    "add_labels(stressed_bars)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate stress testing report\n",
    "stress_dir = '../reports/stress_testing'\n",
    "os.makedirs(stress_dir, exist_ok=True)\n",
    "\n",
    "report_path = os.path.join(stress_dir, f'stress_test_report_{datetime.now().strftime(\"%Y%m%d\")}.md')\n",
    "stress_tester.generate_report(report_path)\n",
    "\n",
    "print(f\"Stress test report generated at: {report_path}\")\n",
    "\n",
    "# Generate stress testing visualizations\n",
    "stress_tester.plot_stress_results(stress_dir)\n",
    "print(f\"Stress test visualizations saved to: {stress_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Monitoring\n",
    "\n",
    "Now we'll set up model monitoring to track performance over time and detect data drift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model monitor\n",
    "monitor = ModelMonitor(credit_model.model, config, model_id=\"credit_risk_model_v1\")\n",
    "\n",
    "# Set reference data (training data)\n",
    "monitor.set_reference_data(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create functions to generate data drift for simulation\n",
    "def create_slight_drift(X, y):\n",
    "    \"\"\"Create slight data drift.\"\"\"\n",
    "    X_drift = X.copy()\n",
    "    for col in X_drift.select_dtypes(include=['number']).columns:\n",
    "        X_drift[col] = X_drift[col] * np.random.normal(1, 0.05, size=len(X_drift))\n",
    "    return X_drift, y\n",
    "\n",
    "def create_moderate_drift(X, y):\n",
    "    \"\"\"Create moderate data drift.\"\"\"\n",
    "    X_drift = X.copy()\n",
    "    for col in X_drift.select_dtypes(include=['number']).columns:\n",
    "        X_drift[col] = X_drift[col] * np.random.normal(1.05, 0.1, size=len(X_drift))\n",
    "    \n",
    "    # Introduce some systematic drift in specific columns\n",
    "    if 'income' in X_drift.columns:\n",
    "        X_drift['income'] = X_drift['income'] * 1.15  # Simulate income inflation\n",
    "    \n",
    "    return X_drift, y\n",
    "\n",
    "def create_severe_drift(X, y):\n",
    "    \"\"\"Create severe data drift and target shift.\"\"\"\n",
    "    X_drift = X.copy()\n",
    "    \n",
    "    # Apply severe drift to all numeric columns\n",
    "    for col in X_drift.select_dtypes(include=['number']).columns:\n",
    "        X_drift[col] = X_drift[col] * np.random.normal(1.1, 0.15, size=len(X_drift))\n",
    "    \n",
    "    # Introduce dramatic shifts in key variables\n",
    "    if 'income' in X_drift.columns:\n",
    "        X_drift['income'] = X_drift['income'] * 1.3  # Dramatic increase in income\n",
    "    \n",
    "    if 'debt_to_income' in X_drift.columns:\n",
    "        X_drift['debt_to_income'] = X_drift['debt_to_income'] * 1.25  # Higher debt ratios\n",
    "    \n",
    "    # Simulate economic shock affecting performance and data distributions\n",
    "    y_drift = y.copy()\n",
    "    # Increase default rate by 20% if 'debt_to_income' is above median\n",
    "    if 'debt_to_income' in X_drift.columns:\n",
    "        high_risk_idx = X_drift['debt_to_income'] > X_drift['debt_to_income'].median()\n",
    "        y_drift[high_risk_idx] = 1  # 1 is the default label\n",
    "    \n",
    "    return X_drift, y_drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define drift scenarios for monitoring simulation\n",
    "drift_scenarios = {\n",
    "    1: lambda X, y: (X.copy(), y.copy()),  # No drift\n",
    "    2: create_slight_drift,\n",
    "    3: create_slight_drift,\n",
    "    4: create_moderate_drift,\n",
    "    5: create_moderate_drift, \n",
    "    6: create_severe_drift\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run monitoring simulation\n",
    "print(\"Simulating model monitoring over 6 time periods...\")\n",
    "simulation_monitor = simulate_monitoring_over_time(\n",
    "    credit_model.model, \n",
    "    X_train, y_train, \n",
    "    drift_scenarios, \n",
    "    periods=6,\n",
    "    model_id=\"credit_risk_model_v1\",\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display monitoring results\n",
    "print(\"Monitoring Results Summary:\\n\")\n",
    "for i, result in enumerate(simulation_monitor.monitoring_results):\n",
    "    print(f\"Period {i+1}: {result.period}\")\n",
    "    print(f\"  Alert Status: {result.alert_status}\")\n",
    "    print(f\"  AUC: {result.performance_metrics.get('roc_auc', 'N/A')}\")\n",
    "    print(f\"  Number of Alerts: {len(result.alert_details)}\")\n",
    "    \n",
    "    # Print alerts for periods with issues\n",
    "    if result.alert_status != \"OK\" and result.alert_details:\n",
    "        print(\"  Alerts:\")\n",
    "        for alert in result.alert_details:\n",
    "            print(f\"    - {alert['type']}: {alert['message']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot monitoring metrics\n",
    "simulation_monitor.plot_monitoring_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and plot performance over time\n",
    "periods = [result.period for result in simulation_monitor.monitoring_results]\n",
    "auc_values = [result.performance_metrics.get('roc_auc', None) for result in simulation_monitor.monitoring_results]\n",
    "accuracy_values = [result.performance_metrics.get('accuracy', None) for result in simulation_monitor.monitoring_results]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(periods, auc_values, 'o-', label='AUC')\n",
    "plt.plot(periods, accuracy_values, 's-', label='Accuracy')\n",
    "plt.title('Model Performance Over Time')\n",
    "plt.xlabel('Period')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and plot data drift metrics\n",
    "drift_scores = [result.data_drift_metrics.get('overall_drift_score', 0) for result in simulation_monitor.monitoring_results]\n",
    "drifted_feature_counts = [len(result.data_drift_metrics.get('drifted_features', [])) for result in simulation_monitor.monitoring_results]\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "ax1.set_xlabel('Period')\n",
    "ax1.set_ylabel('Overall Drift Score', color='tab:blue')\n",
    "ax1.plot(periods, drift_scores, 'o-', color='tab:blue')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('Number of Drifted Features', color='tab:red')\n",
    "ax2.plot(periods, drifted_feature_counts, 's-', color='tab:red')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "plt.title('Data Drift Over Time')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify top drifted features\n",
    "drifted_features = {}\n",
    "for result in simulation_monitor.monitoring_results:\n",
    "    for feature in result.data_drift_metrics.get('drifted_features', []):\n",
    "        if feature in drifted_features:\n",
    "            drifted_features[feature] += 1\n",
    "        else:\n",
    "            drifted_features[feature] = 1\n",
    "\n",
    "# Sort and display top drifted features\n",
    "top_drifted = sorted(drifted_features.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "top_drifted_df = pd.DataFrame(top_drifted, columns=['Feature', 'Drift Occurrences'])\n",
    "top_drifted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top drifted features\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(top_drifted_df['Feature'], top_drifted_df['Drift Occurrences'], color='salmon')\n",
    "plt.title('Top Drifted Features')\n",
    "plt.xlabel('Number of Periods with Drift')\n",
    "plt.ylabel('Feature')\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate monitoring report and visualizations\n",
    "monitoring_dir = '../reports/monitoring'\n",
    "os.makedirs(monitoring_dir, exist_ok=True)\n",
    "\n",
    "# Save monitoring results\n",
    "simulation_monitor.save_results(monitoring_dir)\n",
    "\n",
    "print(f\"Monitoring results and visualizations saved to: {monitoring_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Retraining Recommendation\n",
    "\n",
    "Based on the monitoring results, we can make a data-driven decision about whether the model needs to be retrained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if retraining is recommended based on monitoring results\n",
    "latest_result = simulation_monitor.monitoring_results[-1]\n",
    "\n",
    "# Count critical and warning alerts in the latest period\n",
    "critical_alerts = sum(1 for alert in latest_result.alert_details if alert['severity'] == 'HIGH')\n",
    "warning_alerts = sum(1 for alert in latest_result.alert_details if alert['severity'] == 'MEDIUM')\n",
    "\n",
    "# Get performance degradation if any\n",
    "performance_degradation = None\n",
    "for alert in latest_result.alert_details:\n",
    "    if alert['type'] == 'PERFORMANCE_DEGRADATION':\n",
    "        performance_degradation = alert\n",
    "        break\n",
    "\n",
    "# Get data drift if any\n",
    "data_drift = None\n",
    "for alert in latest_result.alert_details:\n",
    "    if alert['type'] == 'DATA_DRIFT':\n",
    "        data_drift = alert\n",
    "        break\n",
    "\n",
    "# Make recommendation\n",
    "if latest_result.alert_status == \"CRITICAL\" or critical_alerts > 0:\n",
    "    recommendation = \"Model retraining is REQUIRED. Critical issues detected.\"\n",
    "    action = \"Retrain the model as soon as possible with the latest data.\"\n",
    "elif latest_result.alert_status == \"WARNING\" or warning_alerts > 0:\n",
    "    recommendation = \"Model retraining is RECOMMENDED. Multiple warning alerts detected.\"\n",
    "    action = \"Plan for model retraining and review the affected features.\"\n",
    "else:\n",
    "    recommendation = \"Model retraining is NOT necessary at this time.\"\n",
    "    action = \"Continue monitoring the model performance.\"\n",
    "\n",
    "# Display recommendation\n",
    "print(\"Model Retraining Recommendation:\\n\")\n",
    "print(recommendation)\n",
    "print(f\"Recommended Action: {action}\")\n",
    "print(\"\\nBasis for recommendation:\")\n",
    "print(f\"- Alert Status: {latest_result.alert_status}\")\n",
    "print(f\"- Critical Alerts: {critical_alerts}\")\n",
    "print(f\"- Warning Alerts: {warning_alerts}\")\n",
    "\n",
    "if performance_degradation:\n",
    "    print(f\"- Performance Issue: {performance_degradation['message']}\")\n",
    "    \n",
    "if data_drift:\n",
    "    print(f\"- Data Drift Issue: {data_drift['message']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary\n",
    "\n",
    "In this notebook, we demonstrated two critical components of model risk management for credit risk models:\n",
    "\n",
    "1. **Stress Testing**: We evaluated the model's performance under adverse economic scenarios, identified vulnerable customer segments, and calculated capital requirements under stress. This helps ensure the bank is prepared for economic downturns and maintains adequate capital reserves.\n",
    "\n",
    "2. **Model Monitoring**: We simulated monitoring the model over time, detecting performance degradation and data drift. This ongoing monitoring is essential for maintaining model effectiveness and complying with regulatory requirements.\n",
    "\n",
    "Together, these practices form a robust framework for model risk management that aligns with regulatory expectations for banks and financial institutions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
